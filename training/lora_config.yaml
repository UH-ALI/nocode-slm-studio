base_model: google/flan-t5-small

lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  target_modules:
    - q
    - v
  bias: none
  task_type: SEQ_2_SEQ_LM

training:
  num_train_epochs: 15
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1e-3
  warmup_steps: 50
  logging_steps: 10
  save_strategy: "no"
  fp16: true
  max_seq_length: 512

output_dir: adapters/flan_t5_ecommerce_lora

